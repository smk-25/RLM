{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1016c2a1-e094-4b46-8782-40d5e9670a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Agent...\n",
      "\n",
      "Training: 0.0% Complete\n",
      "Training: 1.6% Complete\n",
      "Training: 3.2% Complete\n",
      "Training: 4.8% Complete\n",
      "Training: 6.4% Complete\n",
      "Training: 8.0% Complete\n",
      "Training: 9.6% Complete\n",
      "Training: 11.2% Complete\n",
      "Training: 12.8% Complete\n",
      "Training: 14.4% Complete\n",
      "Training: 16.0% Complete\n",
      "Training: 17.6% Complete\n",
      "Training: 19.2% Complete\n",
      "Training: 20.8% Complete\n",
      "Training: 22.4% Complete\n",
      "Training: 24.0% Complete\n",
      "Training: 25.6% Complete\n",
      "Training: 27.2% Complete\n",
      "Training: 28.8% Complete\n",
      "Training: 30.4% Complete\n",
      "Training: 32.0% Complete\n",
      "Training: 33.6% Complete\n",
      "Training: 35.2% Complete\n",
      "Training: 36.8% Complete\n",
      "Training: 38.4% Complete\n",
      "Training: 40.0% Complete\n",
      "Training: 41.6% Complete\n",
      "Training: 43.2% Complete\n",
      "Training: 44.8% Complete\n",
      "Training: 46.4% Complete\n",
      "Training: 48.0% Complete\n",
      "Training: 49.6% Complete\n",
      "Training: 51.2% Complete\n",
      "Training: 52.8% Complete\n",
      "Training: 54.4% Complete\n",
      "Training: 56.0% Complete\n",
      "Training: 57.6% Complete\n",
      "Training: 59.2% Complete\n",
      "Training: 60.8% Complete\n",
      "Training: 62.4% Complete\n",
      "Training: 64.0% Complete\n",
      "Training: 65.6% Complete\n",
      "Training: 67.2% Complete\n",
      "Training: 68.8% Complete\n",
      "Training: 70.4% Complete\n",
      "Training: 72.0% Complete\n",
      "Training: 73.6% Complete\n",
      "Training: 75.2% Complete\n",
      "Training: 76.8% Complete\n",
      "Training: 78.4% Complete\n",
      "Training: 80.0% Complete\n",
      "Training: 81.6% Complete\n",
      "Training: 83.2% Complete\n",
      "Training: 84.8% Complete\n",
      "Training: 86.4% Complete\n",
      "Training: 88.0% Complete\n",
      "Training: 89.6% Complete\n",
      "Training: 91.2% Complete\n",
      "Training: 92.8% Complete\n",
      "Training: 94.4% Complete\n",
      "Training: 96.0% Complete\n",
      "Training: 97.6% Complete\n",
      "Training: 99.2% Complete\n",
      "Training Complete\n",
      "\n",
      "Begining validation of the agent trained by pairing it to an agent with a random approach...\n",
      "\n",
      "Validation results:\n",
      "Trained Agent Wins: 52.8%;     Random Action Player Wins: 34.4%;      Draws:12.8%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "LENGTH = 3\n",
    "NUM_ROWS = LENGTH\n",
    "NUM_COLS = LENGTH\n",
    "NUM_STATES = 3**(NUM_ROWS*NUM_COLS) # Num states can be x,o or empty for each position in the board\n",
    "\n",
    "class Environment:\n",
    "\tdef __init__(self, length_r = NUM_ROWS, length_c = NUM_COLS,print_board=False):\n",
    "\t\tself.board = np.zeros((length_r,length_c)); # Game board, init as empty\n",
    "\t\tself.x = -1 # player 1\n",
    "\t\tself.o = 1 # player 2\n",
    "\t\tself.winner = None\n",
    "\t\tself.ended = False\n",
    "\t\tself.print_board = print_board # Defines if board is printed doring games\n",
    "\n",
    "\tdef is_empty(self,i,j):\n",
    "\t\treturn self.board[i,j] == 0\n",
    "\n",
    "\tdef reward(self, player):\n",
    "\t\tif not self.ended or self.winner is None:\n",
    "\t\t\treturn 0\n",
    "\t\telif self.winner == player:\n",
    "\t\t\treturn 1\n",
    "\t\telse:\n",
    "\t\t\treturn -1\n",
    "\n",
    "\tdef get_state(self):\n",
    "\t\t# returns the current state, represented as an int\n",
    "\t\tcell = 0\n",
    "\t\tstate = 0\n",
    "\n",
    "\t\tfor i in range(NUM_ROWS):\n",
    "\t\t\tfor j in range(NUM_COLS):\n",
    "\t\t\t\tif self.board[i,j] == self.x:\n",
    "\t\t\t\t\tval = 1\n",
    "\t\t\t\telif self.board[i,j] == self.o:\n",
    "\t\t\t\t\tval = 2\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tval = 0\n",
    "\n",
    "\t\t\t\tstate += (3^cell)*val\n",
    "\t\t\t\tcell+=1\n",
    "\n",
    "\t\treturn state\n",
    "\n",
    "\tdef is_game_over(self, recalculate=False):\n",
    "\t\tif not recalculate and self.ended:\n",
    "\t\t\treturn self.ended\n",
    "\t\t# check rows\n",
    "\t\tfor i in range(NUM_ROWS):\n",
    "\t\t\tfor player in (self.x, self.o):\n",
    "\t\t\t\tif self.board[i].sum() == player*NUM_ROWS:\n",
    "\t\t\t\t\tself.winner = player\n",
    "\t\t\t\t\tself.ended = True\n",
    "\t\t\t\t\treturn True\n",
    "\n",
    "\t\t# check columns\n",
    "\t\tfor j in range(NUM_COLS):\n",
    "\t\t\tfor player in (self.x, self.o):\n",
    "\t\t\t\tif self.board[:,j].sum() == player*NUM_COLS:\n",
    "\t\t\t\t\tself.winner = player\n",
    "\t\t\t\t\tself.ended = True\n",
    "\t\t\t\t\treturn True\n",
    "\n",
    "\t\t# check diagonals\n",
    "\t\tfor player in (self.x, self.o):\n",
    "\t\t  # top-left -> bottom-right diagonal\n",
    "\t\t\tif self.board.trace() == player*LENGTH:\n",
    "\t\t\t\tself.winner = player\n",
    "\t\t\t\tself.ended = True\n",
    "\t\t\t\treturn True\n",
    "\t\t  # top-right -> bottom-left diagonal\n",
    "\t\t\tif np.fliplr(self.board).trace() == player*LENGTH:\n",
    "\t\t\t\tself.winner = player\n",
    "\t\t\t\tself.ended = True\n",
    "\t\t\t\treturn True\n",
    "\n",
    "\t\t# check if draw\n",
    "\t\tif np.all((self.board == 0) == False):\n",
    "\t\t  # winner stays None\n",
    "\t\t\tself.winner = None\n",
    "\t\t\tself.ended = True\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\t# game is not over\n",
    "\t\tself.winner = None\n",
    "\t\treturn False\n",
    "\n",
    "\tdef is_draw(self):\n",
    "\t\treturn self.ended and self.winner is None\n",
    "\n",
    "\tdef draw_board(self):\n",
    "\t\tprint()\n",
    "\t\tfor i in range(NUM_ROWS):\n",
    "\t\t\tfor j in range(NUM_COLS):\n",
    "\t\t\t\t# Print column dividers\n",
    "\t\t\t\tif j > 0:\n",
    "\t\t\t\t\tprint('|',end='')\n",
    "\n",
    "\t\t\t\t# Gets each block value\n",
    "\t\t\t\tif self.board[i,j] == self.x:\n",
    "\t\t\t\t\tprint('X',end='')\n",
    "\t\t\t\telif self.board[i,j] == self.o:\n",
    "\t\t\t\t\tprint('O',end='')\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(' ',end='')\n",
    "\n",
    "\t\t\t# Prints line divider\n",
    "\t\t\tif i < NUM_ROWS-1:\n",
    "\t\t\t\tprint()\n",
    "\t\t\t\tprint('-+-+-')\n",
    "\n",
    "\t\tprint()\n",
    "\n",
    "\tdef play_game(self,p1,p2):\n",
    "\t\tcurrent_player = None\n",
    "\t\t#loop until game is over\n",
    "\t\twhile not self.is_game_over():\n",
    "\t\t\tif current_player == p1:\n",
    "\t\t\t\tcurrent_player = p2\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_player = p1\n",
    "\n",
    "\t\t\tif self.print_board: # draw the board before the player's action\n",
    "\t\t\t\tself.draw_board()\n",
    "\n",
    "\t\t\t# takes action\n",
    "\t\t\tcurrent_player.take_action(env=self)\n",
    "\n",
    "\t\t\t# update state history for both players\n",
    "\t\t\tstate = self.get_state()\n",
    "\t\t\tp1.update_state_history(state)\n",
    "\t\t\tp2.update_state_history(state)\n",
    "\n",
    "\t\tif self.print_board: # draw the final state of the board\n",
    "\t\t\tself.draw_board()\n",
    "\n",
    "\t\t# updates value function\n",
    "\t\tp1.update_value_function(env=self)\n",
    "\t\tp2.update_value_function(env=self)\n",
    "\n",
    "\t\tif self.print_board:\n",
    "\t\t\tprint()\n",
    "\t\t\tif self.winner is None:\n",
    "\t\t\t\tprint(\"It is a draw!\")\n",
    "\t\t\telif self.winner == self.x:\n",
    "\t\t\t\tprint(\"X player won!\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"O player won!\")\n",
    "\t\t\tprint()\n",
    "\n",
    "class Agent:\n",
    "\tdef __init__(self, sym = 1, eps=0.1, alpha=0.8, states_results=None):\n",
    "\t\tself.eps = eps # probability of choosing random action\n",
    "\t\tself.alpha = alpha # learning rate\n",
    "\t\tself.state_history = []\n",
    "\t\tself.sym = sym\n",
    "\t\tif states_results is not None:\n",
    "\t\t\tself.init_value_function(states_results)\n",
    "\n",
    "\tdef set_eps(self,eps):\n",
    "\t\tself.eps = eps\n",
    "\n",
    "\tdef init_value_function(self, states_results):\n",
    "\t\t# initialize value function\n",
    "\t\t# states_results is an array of state_winner_tripples\n",
    "\t\tself.value_fun = np.zeros(NUM_STATES)\n",
    "\n",
    "\t\tfor state, ended, winner in states_results:\n",
    "\t\t\tval = 0\n",
    "\t\t\tif ended:\n",
    "\t\t\t\tif winner == self.sym:\n",
    "\t\t\t\t\tval = 1\n",
    "\t\t\t\telif winner is None:\n",
    "\t\t\t\t\tval = 0.5\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tval = -1\n",
    "\t\t\telse:\n",
    "\t\t\t\tval = 0\n",
    "\n",
    "\t\t\tself.value_fun[state] = val\n",
    "\n",
    "\t\treturn self.value_fun\n",
    "\n",
    "\tdef update_value_function(self, env):\n",
    "\t\t# we want to BACKTRACK over the states, so that:\n",
    "\t\t# V(prev_state) = V(prev_state) + alpha*(V(next_state) - V(prev_state))\n",
    "\t\t# where V(next_state) = reward if it's the most current state\n",
    "\t\t#\n",
    "\t\t# NOTE: we ONLY do this at the end of an episode\n",
    "\n",
    "\t\tif not env.ended:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\treward = env.reward(self.sym)\n",
    "\t\tvalue = reward\n",
    "\t\tfor prev in reversed(self.state_history):\n",
    "\t\t\tself.value_fun[prev] += self.alpha*(value - self.value_fun[prev])\n",
    "\t\t\tvalue = self.value_fun[prev]\n",
    "\t\tself.reset_history()\n",
    "\n",
    "\tdef reset_history(self):\n",
    "\t\tself.state_history = []\n",
    "\n",
    "\tdef update_state_history(self, state):\n",
    "\t\t# cannot put this in take_action, because take_action only happens\n",
    "\t\t# once every other iteration for each player\n",
    "\t\t# state history needs to be updated every iteration\n",
    "\t\t# s = env.get_state() # don't want to do this twice so pass it in\n",
    "\t\tself.state_history.append(state)\n",
    "\n",
    "\tdef take_action(self,env):\n",
    "\t\tif env.print_board:\n",
    "\t\t\tprint()\n",
    "\t\t\tprint('Agent action:')\n",
    "\t\t# choose an action based on epsilon-greedy strategy\n",
    "\t\tr = np.random.rand()\n",
    "\t\tbest_move = None # It is the number corresponding to the best next stage\n",
    "\n",
    "\t\t# get all possible valid actions\n",
    "\t\tvalid_actions = []\n",
    "\t\tfor i in range(NUM_ROWS):\n",
    "\t\t\tfor j in range(NUM_COLS):\n",
    "\t\t\t\tif env.is_empty(i,j): # If cell is empty then it is a valid action\n",
    "\t\t\t\t\tvalid_actions.append((i,j))\n",
    "\n",
    "\t\taction = None\n",
    "\t\tif r < self.eps: # takes a random action\n",
    "\t\t\t# Chooses a random action based on the possible available actions\n",
    "\t\t\tidx = np.random.choice(len(valid_actions))\n",
    "\t\t\taction = valid_actions[idx]\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# choose the best action based on current values of states\n",
    "\t\t\t# loop through all possible moves, get their values\n",
    "\t\t\t# keep track of the best value\n",
    "\t\t\tbest_value = -10000\n",
    "\t\t\tfor val_act in valid_actions:\n",
    "\t\t\t\tenv.board[val_act[0],val_act[1]] = self.sym\n",
    "\t\t\t\tstate = env.get_state()\n",
    "\t\t\t\tenv.board[val_act[0],val_act[1]] = 0\n",
    "\n",
    "\t\t\t\tif self.value_fun[state] > best_value:\n",
    "\t\t\t\t\taction = val_act\n",
    "\t\t\t\t\tbest_value = self.value_fun[state]\n",
    "\t\t\t\t\tbest_move = state\n",
    "\n",
    "\t\t# takes the action\n",
    "\t\tif action is not None:\n",
    "\t\t\tenv.board[action[0],action[1]] = self.sym\n",
    "\n",
    "class Random_Action_Player:\n",
    "\tdef __init__(self,sym):\n",
    "\t\tself.sym = sym\n",
    "\n",
    "\tdef take_action(self, env):\n",
    "\t\t# get all possible valid actions\n",
    "\t\tvalid_actions = []\n",
    "\t\tfor i in range(NUM_ROWS):\n",
    "\t\t\tfor j in range(NUM_COLS):\n",
    "\t\t\t\tif env.is_empty(i,j): # If cell is empty then it is a valid action\n",
    "\t\t\t\t\tvalid_actions.append((i,j))\n",
    "\n",
    "\t\tidx = np.random.choice(len(valid_actions))\n",
    "\t\taction = valid_actions[idx]\n",
    "\t\tenv.board[action[0],action[1]] = self.sym\n",
    "\n",
    "\tdef update_value_function(self, env):\n",
    "\t\tpass\n",
    "\n",
    "\tdef update_state_history(self, state):\n",
    "\t\tpass\n",
    "\n",
    "class Default_Player:\n",
    "\tdef __init__(self,sym):\n",
    "\t\tself.sym = sym\n",
    "\n",
    "\tdef take_action(self, env):\n",
    "\t\twhile True:\n",
    "\t\t\t# break if we make a legal move\n",
    "\t\t\tprint()\n",
    "\t\t\tmove = input(\"Enter coordinates i,j for your next move (i,j=[0...2]): \")\n",
    "\t\t\ti, j = move.split(',')\n",
    "\t\t\ti = int(i)\n",
    "\t\t\tj = int(j)\n",
    "\t\t\tif env.is_empty(i, j):\n",
    "\t\t\t\tenv.board[i,j] = self.sym\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint()\n",
    "\t\t\t\tprint('Move is invalid. Type the coordinates (i,j) of an empty cell in the board.')\n",
    "\n",
    "\tdef update_value_function(self, env):\n",
    "\t\tpass\n",
    "\n",
    "\tdef update_state_history(self, state):\n",
    "\t\tpass\n",
    "\n",
    "# Gets initial values for every possible state\n",
    "def get_initial_states_results(env, i=0, j=0):\n",
    "\t# recursive function that will return all\n",
    "\t# possible states (as ints) and who the corresponding winner is for those states (if any)\n",
    "\t\n",
    "\tresults = [] # results as an array of state_winner_triples\n",
    "\n",
    "\tfor val in (0,env.x,env.o):\n",
    "\t\tenv.board[i,j] = val\n",
    "\n",
    "\t\tif j == NUM_COLS - 1:\n",
    "\t\t\t# if j = NUM_COLS and i < NUM_ROWS then j=0 and i+=1\n",
    "\t\t\tif i == NUM_ROWS - 1:\n",
    "\t\t\t\t# break point\n",
    "\t\t\t\t# get the results for a state\n",
    "\t\t\t\tstate = env.get_state()\n",
    "\t\t\t\tended = env.is_game_over(recalculate=True)\n",
    "\t\t\t\twinner = env.winner\n",
    "\t\t\t\tresults.append((state,ended,winner))\n",
    "\t\t\telse:\n",
    "\t\t\t\tresults += get_initial_states_results(env, i+1, 0)\n",
    "\t\telse:\n",
    "\t\t\tresults += get_initial_states_results(env, i, j+1)\n",
    "\n",
    "\treturn results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t# init the environment\n",
    "\te = Environment()\n",
    "\n",
    "\t# get state values before training\n",
    "\tstates_results = get_initial_states_results(Environment())\n",
    "\n",
    "\t# init the players as agents\n",
    "\ta1 = Agent(eps = 1, sym = e.x, states_results=states_results)\n",
    "\ta2 = Agent(sym = e.o, states_results=states_results)\n",
    "\n",
    "\tprint('Training Agent...')\n",
    "\tprint()\n",
    "\tT = 50000 # number of games that will be played for training\n",
    "\tfor t in range(T):\n",
    "\t\tif t%800==0:\n",
    "\t\t\tprint('Training: {}% Complete'.format(t*100/T))\n",
    "\t\t\n",
    "\t\tif t < 1000: # min eps=0.01\n",
    "\t\t\ta1.set_eps(1/(t+1))\n",
    "\t\t\ta2.set_eps(1/(t+1))\n",
    "\n",
    "\t\tEnvironment().play_game(a1,a2)\n",
    "\n",
    "\tprint('Training Complete')\n",
    "\tprint()\n",
    "\n",
    "\ta1.set_eps(0)\n",
    "\ta2.set_eps(0)\n",
    "\n",
    "\tprint('Begining validation of the agent trained by pairing it to an agent with a random approach...')\n",
    "\tprint()\n",
    "\n",
    "\trand_p = Random_Action_Player(e.o)\n",
    "\n",
    "\tG = 1000 # Number of games played for validation\n",
    "\tagent_wins = 0\n",
    "\trand_player_wins = 0\n",
    "\tdraws = 0\n",
    "\tfor g in range(G):\n",
    "\t\tval_env = Environment()\n",
    "\t\tval_env.play_game(a1,rand_p)\n",
    "\n",
    "\t\tif val_env.winner == a1.sym:\n",
    "\t\t\tagent_wins += 1\n",
    "\t\telif val_env.winner == rand_p.sym:\n",
    "\t\t\trand_player_wins += 1\n",
    "\t\telse:\n",
    "\t\t\tdraws += 1\n",
    "\n",
    "\tprint('Validation results:')\n",
    "\tprint('Trained Agent Wins: {}%;     Random Action Player Wins: {}%;      Draws:{}%'.format(agent_wins*100/G,rand_player_wins*100/G,draws*100/G))\n",
    "\n",
    "\t# print('Begining game against human player. Human is \"X\", angent is \"O\":')\n",
    "\t# print()\n",
    "\n",
    "\t# human = Default_Player(e.o)\n",
    "\n",
    "\t# while True:\n",
    "\t# \tEnvironment(print_board=True).play_game(a1,human)\n",
    "\t# \tprint()\n",
    "\t# \tanswer = input(\"Play again? [y/n]: \")\n",
    "\t# \tif answer and answer.lower()[0] == 'n':\n",
    "\t# \t\tbreak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662cf55-ac61-4f4c-942f-8bc99a9a0a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
